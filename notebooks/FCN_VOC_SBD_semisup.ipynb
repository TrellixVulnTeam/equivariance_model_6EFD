{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZbrYjvKvkaH"
   },
   "source": [
    "# FCN VOC 2012 and SBD Semi Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KX2t25zgvkaf"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torch.utils.data as tud\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from sklearn.metrics import jaccard_score\n",
    "import pickle\n",
    "import my_datasets as mdset\n",
    "import eval_train as ev\n",
    "from utils import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aw2xTNEkvka0"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IzkSq_mgrcLR"
   },
   "outputs": [],
   "source": [
    "#MODEL SAVE AND LOAD \n",
    "dataroot_voc = '/data/voc2012'\n",
    "dataroot_sbd = '/data/sbd'\n",
    "model_name = 'fcn_voc_sbd30_semisup_g09_KL' # Model name for save during semisup training\n",
    "SAVE_DIR = '/data/model'\n",
    "file_base = None # Base model \n",
    "fcn= True\n",
    "pretrained=False\n",
    "save_all_ep = True # A save for each epoch \n",
    "save = os.path.join(SAVE_DIR,model_name)\n",
    "# MODEL PARAMETERS \n",
    "desactivate_bn = False # Desactive BatchNorm for fine tune models. \n",
    "# TRAIN PARAMETERS\n",
    "batch_size = 2\n",
    "gamma = 0.9\n",
    "learning_rate = 10e-4\n",
    "moment = 0.9\n",
    "wd = 2e-4\n",
    "n_epochs = 25 # N_epoch for multi_task/semi sup training\n",
    "n_epochs_supervised = 20 # Train in fully supervised for n_epoch_supervised\n",
    "angle_max = 30\n",
    "\n",
    "# LOSS \n",
    "\n",
    "#criterion_unsupervised = nn.L1Loss(reduction='mean')\n",
    "criterion_unsupervised = nn.KLDivLoss(reduction = 'batchmean', log_target = False)\n",
    "#criterion_unsupervised = nn.CrossEntropyLoss(ignore_index=21)\n",
    "criterion_supervised = nn.CrossEntropyLoss(ignore_index=21) # On ignore la classe border.\n",
    "Loss = 'KL' # Loss = 'KL' or 'CE' or None for L1,MSEâ€¦\n",
    "\n",
    "# DATASET AND DATA AUG \n",
    "rotate = False # random rotation during training\n",
    "split = True # split the supervised dataset\n",
    "rate = 0.3 # percent of data of VOC + SBD we use for supervised dataset\n",
    "fully_supervised = False # Use the same dataloader for equivariance loss and supervised loss\n",
    "\n",
    "# BEFORE TRAIN IN SEMISUP/MULTITASK\n",
    "pretrain_model = False  # Pretrain the model in a fully supervised way before appli equiv loss\n",
    "eval_before_semisup = False # Eval the loaded model before training with equiv_loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "AAj55UZxOZte",
    "outputId": "9257ab6a-d935-4c69-ddf6-ea40effa907a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n"
     ]
    }
   ],
   "source": [
    "train_dataset_VOC = mdset.VOCSegmentation(dataroot_voc,year='2012', image_set='train', download=True,rotate=rotate)\n",
    "val_dataset_VOC = mdset.VOCSegmentation(dataroot_voc,year='2012', image_set='val', download=True)\n",
    "train_dataset_SBD = mdset.SBDataset(dataroot_sbd, image_set='train_noval',mode='segmentation',rotate=rotate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatene Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_unsup = tud.ConcatDataset([train_dataset_VOC,train_dataset_SBD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split:\n",
    "    train_dataset_sup = split_dataset(train_dataset_unsup,rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LUqeDj5hvka3",
    "outputId": "92a782a5-3bc5-48f5-b57a-af5993c9c201"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "if fully_supervised : \n",
    "    train_dataset_unsup = train_dataset_sup\n",
    "\n",
    "dataloader_train_VOC = torch.utils.data.DataLoader(train_dataset_VOC, batch_size=batch_size,\\\n",
    "                                                       shuffle=True,drop_last=True)\n",
    "dataloader_train_sup = torch.utils.data.DataLoader(train_dataset_sup, batch_size=batch_size,\\\n",
    "                                                       shuffle=True,drop_last=True)\n",
    "dataloader_train_equiv = torch.utils.data.DataLoader(train_dataset_unsup,batch_size=batch_size,\\\n",
    "                                                     shuffle=True,drop_last=True)\n",
    "\n",
    "dataloader_val = torch.utils.data.DataLoader(val_dataset_VOC, batch_size=batch_size)\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device :\",device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "63IQP70Qg8BL",
    "outputId": "e38a98ff-0b99-4e3a-eae4-bdf29ddf3636",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille dataset train supervised : 2126\n",
      "Taille dataset train unsupervised : 7087\n",
      "Taille dataset val VOC : 1449\n"
     ]
    }
   ],
   "source": [
    "print(\"Taille dataset train supervised :\",len(train_dataset_sup))\n",
    "print(\"Taille dataset train unsupervised :\",len(train_dataset_unsup))\n",
    "print(\"Taille dataset val VOC :\",len(val_dataset_VOC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tl0nR2NJ8I2i"
   },
   "source": [
    "\n",
    "## Load Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(file=None,fcn=False,pretrained=False):\n",
    "    if file is None:\n",
    "        if fcn is False:\n",
    "            model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=pretrained)\n",
    "        else:\n",
    "            model = torchvision.models.segmentation.fcn_resnet101(pretrained=pretrained)\n",
    "    else:\n",
    "        model = torch.load(os.path.join(SAVE_DIR,file))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(file=file_base,fcn=fcn,pretrained=pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "colab_type": "code",
    "id": "N3HOA_uv8Oyd",
    "outputId": "c886c10b-558e-4acb-a2ac-b2110fddc29f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCN(\n",
       "  (backbone): IntermediateLayerGetter(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (6): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (7): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (8): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (9): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (10): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (11): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (12): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (13): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (14): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (15): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (16): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (17): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (18): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (19): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (20): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (21): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (22): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): FCNHead(\n",
       "    (0): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Conv2d(512, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NQeaeCk1hsGu"
   },
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_train = []\n",
    "iou_test = []\n",
    "combine_loss_train = []\n",
    "combine_loss_test = []\n",
    "loss_train_unsup = []\n",
    "loss_train_sup = []\n",
    "loss_test = []\n",
    "loss_test_unsup = []\n",
    "pix_accuracy_train = []\n",
    "pix_accuracy_test = []\n",
    "accuracy_test = []\n",
    "accuracy_train = []\n",
    "#\n",
    "all_combine_loss_train = []\n",
    "all_loss_train_sup = []\n",
    "all_loss_train_unsup = []\n",
    "all_iou_train= []\n",
    "all_pix_accuracy =  []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=moment, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain fully supervised "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pretrain the model in fully supervised\n",
    "if pretrain_model:\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    for ep in range(n_epochs_supervised):\n",
    "        \n",
    "        print(\"EPOCH\",ep)\n",
    "        model.train()\n",
    "        if desactivate_bn : \n",
    "            model.apply(deactivate_batchnorm) # Disable BN \n",
    "        for i,(x,mask) in enumerate(dataloader_train_sup):\n",
    "            x = x.to(device)\n",
    "            mask = mask.to(device)  \n",
    "            pred = model(x)\n",
    "            pred = pred[\"out\"]\n",
    "            loss = criterion_supervised(pred,mask)\n",
    "            all_loss_train_sup.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            all_iou_train.append(inter_over_union(pred.argmax(dim=1).detach().cpu(),mask.detach().cpu()))\n",
    "            optimizer.step()        \n",
    "\n",
    "        #lr_scheduler.step()\n",
    "        m_iou = np.array(all_iou_train).mean()\n",
    "        m_loss = np.array(all_loss_train_sup).mean()\n",
    "        loss_train_sup.append(m_loss)\n",
    "        iou_train.append(m_iou)\n",
    "        all_loss_train_sup = []\n",
    "        all_iou_train = []\n",
    "        print(\"EP:\",ep,\" loss train:\",m_loss,\" iou train:\",m_iou)\n",
    "\n",
    "        #Eval model\n",
    "\n",
    "        model.eval()\n",
    "        state = ev.eval_model(model,dataloader_val,device=device,num_classes=21)\n",
    "        iou = state.metrics['mean IoU']\n",
    "        acc = state.metrics['accuracy']\n",
    "        loss = state.metrics['CE Loss'] \n",
    "        loss_test.append(loss)\n",
    "        iou_test.append(iou)\n",
    "        accuracy_test.append(acc)\n",
    "        print('EP:',ep,'iou:',state.metrics['mean IoU'],\\\n",
    "              'Accuracy:',state.metrics['accuracy'],'Loss CE',state.metrics['CE Loss'])\n",
    "\n",
    "        torch.save(model,save)\n",
    "        ## Save model\n",
    "        if save_all_ep:\n",
    "            save_model = model_name+'_pretrain_sup'+'_ep'+str(ep)+'.pt'\n",
    "            save = os.path.join(SAVE_DIR,save_model)\n",
    "            torch.save(model,save)\n",
    "        else:\n",
    "            save_model = model_name+'_pretrain_sup'+'.pt'\n",
    "            save = os.path.join(SAVE_DIR,save_model)\n",
    "            torch.save(model,save)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_before_semisup:\n",
    "    model.eval()\n",
    "    print(\"EVAL ON TRAIN VOC 2012:\")\n",
    "    m_pix_acc, m_loss_equiv = eval_accuracy_equiv(model,dataloader_train_VOC,criterion=criterion_unsupervised,\\\n",
    "                            nclass=21,device=device,Loss=Loss,plot=False,angle_max=30,random_angle=False) \n",
    "    d_iou = ev.eval_model_all_angle(model,train=True,batch_size=batch_size,device=device,num_classes=21)\n",
    "    print(\"EVAL ON VAL VOC 2012:\")\n",
    "    m_pix_acc, m_loss_equiv = eval_accuracy_equiv(model,dataloader_val,criterion=criterion_unsupervised,\\\n",
    "                            nclass=21,device=device,Loss=Loss,plot=False,angle_max=30,random_angle=False) \n",
    "    d_iou = ev.eval_model_all_angle(model,train=False,batch_size=batch_size,device=device,num_classes=21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2*learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi sup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5Y_Uq45Mh0kb",
    "outputId": "028dcf3b-6255-4f1a-dba0-b8ddd0179513"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/utils.py:143: RuntimeWarning: invalid value encountered in true_divide\n",
      "  acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
      "/workspace/utils.py:145: RuntimeWarning: invalid value encountered in true_divide\n",
      "  iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss sup : 0.3591289222240448 loss unsup : 0.17465469241142273 loss : 0.3406814932823181 iou train : 0.4209333004030653\n",
      "EP: 0  combine loss train: 0.8921749784715425  pixel accuracy between masks  0.9156482713200325\n",
      "EP: 0 iou: 0.4209333004030653 Accuracy: 0.865992344240672 Loss CE 0.4409552488740845\n",
      "Mean Pixel Accuracy between masks : 0.9229804682717814 Loss Validation : 0.16686472\n",
      "VOC Dataset Train\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Scores for datasets rotate by 0 degrees:\n",
      "   mIoU 0.43223001278648543 Accuracy 0.868306379741003 CE Loss 0.43560440783119186\n",
      "Scores for datasets rotate by 10 degrees:\n",
      "   mIoU 0.36041602212583496 Accuracy 0.8566361411549779 CE Loss 0.4976224642491243\n",
      "Scores for datasets rotate by 20 degrees:\n",
      "   mIoU 0.324560525247866 Accuracy 0.850466231468458 CE Loss 0.5330282092511736\n",
      "Scores for datasets rotate by 30 degrees:\n",
      "   mIoU 0.3142540319302799 Accuracy 0.8496308469339089 CE Loss 0.5370120946826831\n",
      "Scores for datasets rotate by 330 degrees:\n",
      "   mIoU 0.3063900888305424 Accuracy 0.8481012545672066 CE Loss 0.5472459291448929\n",
      "Scores for datasets rotate by 340 degrees:\n",
      "   mIoU 0.32053185195111994 Accuracy 0.850457789209267 CE Loss 0.5350357731549792\n",
      "Scores for datasets rotate by 350 degrees:\n",
      "   mIoU 0.36224887848440335 Accuracy 0.8574257879441911 CE Loss 0.49857039040188644\n",
      "VOC Dataset Val\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Scores for datasets rotate by 0 degrees:\n",
      "   mIoU 0.4209333004030653 Accuracy 0.865992344240672 CE Loss 0.4409552483341873\n",
      "Scores for datasets rotate by 10 degrees:\n",
      "   mIoU 0.3513492259774929 Accuracy 0.8548394041391533 CE Loss 0.5055908077673471\n",
      "Scores for datasets rotate by 20 degrees:\n",
      "   mIoU 0.31811614421302836 Accuracy 0.8486628607364394 CE Loss 0.539473106427757\n",
      "Scores for datasets rotate by 30 degrees:\n",
      "   mIoU 0.3066284953879716 Accuracy 0.8467987103940281 CE Loss 0.5437500664464271\n",
      "Scores for datasets rotate by 330 degrees:\n",
      "   mIoU 0.29905964582667893 Accuracy 0.8451048111123362 CE Loss 0.5515468207626938\n",
      "Scores for datasets rotate by 340 degrees:\n",
      "   mIoU 0.30946587832090094 Accuracy 0.8462129258701165 CE Loss 0.5428411942656653\n",
      "Scores for datasets rotate by 350 degrees:\n",
      "   mIoU 0.35092920113611104 Accuracy 0.8541319763476954 CE Loss 0.5063621099958591\n",
      "EPOCH 1\n",
      "loss sup : 0.5335169434547424 loss unsup : 0.10249460488557816 loss : 0.49041470885276794 iou train : 0.48552635949138623\n",
      "EP: 1  combine loss train: 0.6848121217170753  pixel accuracy between masks  0.9164963199507422\n",
      "EP: 1 iou: 0.48552635949138623 Accuracy: 0.875757258117208 Loss CE 0.4266078374853128\n",
      "EPOCH 2\n",
      "loss sup : 0.4031848609447479 loss unsup : 0.09744174033403397 loss : 0.37261053919792175 iou train : 0.4589846001472841\n",
      "EP: 2  combine loss train: 0.6186371872364521  pixel accuracy between masks  0.9151958841264242\n",
      "EP: 2 iou: 0.4589846001472841 Accuracy: 0.862775279115162 Loss CE 0.5103374810425308\n",
      "EPOCH 3\n",
      "loss sup : 0.3368697762489319 loss unsup : 0.18896141648292542 loss : 0.3220789134502411 iou train : 0.4817458445553928\n",
      "EP: 3  combine loss train: 0.5513374421987349  pixel accuracy between masks  0.9177053098378541\n",
      "EP: 3 iou: 0.4817458445553928 Accuracy: 0.8792232145409403 Loss CE 0.4073715359049678\n",
      "Mean Pixel Accuracy between masks : 0.9169844011426238 Loss Validation : 0.16836981\n",
      "EPOCH 4\n",
      "loss sup : 0.5033448934555054 loss unsup : 0.03864460438489914 loss : 0.4568748474121094 iou train : 0.4734313318139856\n",
      "EP: 4  combine loss train: 0.5033895993151423  pixel accuracy between masks  0.9169647086011812\n",
      "EP: 4 iou: 0.4734313318139856 Accuracy: 0.8744088046847244 Loss CE 0.4141900361000751\n",
      "EPOCH 5\n",
      "loss sup : 0.4064953625202179 loss unsup : 0.03268711641430855 loss : 0.36911454796791077 iou train : 0.4975851653928271\n",
      "EP: 5  combine loss train: 0.4470228026932069  pixel accuracy between masks  0.9216149263091878\n",
      "EP: 5 iou: 0.4975851653928271 Accuracy: 0.8784050799163675 Loss CE 0.4059176121917447\n",
      "VOC Dataset Train\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Scores for datasets rotate by 0 degrees:\n",
      "   mIoU 0.5653573919109508 Accuracy 0.8954461236338798 CE Loss 0.3555039995187089\n",
      "Scores for datasets rotate by 10 degrees:\n",
      "   mIoU 0.49741470219057976 Accuracy 0.8843874461433375 CE Loss 0.40600211044793816\n",
      "Scores for datasets rotate by 20 degrees:\n",
      "   mIoU 0.4302559116639512 Accuracy 0.8717290893677045 CE Loss 0.45644378332329577\n",
      "Scores for datasets rotate by 30 degrees:\n",
      "   mIoU 0.40462088964274445 Accuracy 0.8672329144032722 CE Loss 0.4729197032754742\n",
      "Scores for datasets rotate by 330 degrees:\n",
      "   mIoU 0.40254332453170116 Accuracy 0.8656722205718305 CE Loss 0.4849831701073124\n",
      "Scores for datasets rotate by 340 degrees:\n",
      "   mIoU 0.43636909942838936 Accuracy 0.8719794521170822 CE Loss 0.4637782059462303\n",
      "Scores for datasets rotate by 350 degrees:\n",
      "   mIoU 0.5043776718232096 Accuracy 0.8854054684216057 CE Loss 0.40187400987605904\n",
      "VOC Dataset Val\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /data/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Scores for datasets rotate by 0 degrees:\n",
      "   mIoU 0.4975851653928271 Accuracy 0.8784050799163675 CE Loss 0.40591761104382057\n",
      "Scores for datasets rotate by 10 degrees:\n",
      "   mIoU 0.4363827682964069 Accuracy 0.8685843174848191 CE Loss 0.46088402056545946\n",
      "Scores for datasets rotate by 20 degrees:\n",
      "   mIoU 0.3856422996929456 Accuracy 0.859541052592892 CE Loss 0.5084316035805899\n",
      "Scores for datasets rotate by 30 degrees:\n",
      "   mIoU 0.3639535548840752 Accuracy 0.8558673626169445 CE Loss 0.5213603809748418\n",
      "Scores for datasets rotate by 330 degrees:\n",
      "   mIoU 0.3616805288331361 Accuracy 0.8543947085523173 CE Loss 0.5281026269225024\n",
      "Scores for datasets rotate by 340 degrees:\n",
      "   mIoU 0.385842223144157 Accuracy 0.8582997379339352 CE Loss 0.5138885853330047\n",
      "Scores for datasets rotate by 350 degrees:\n",
      "   mIoU 0.4432519651932774 Accuracy 0.8691980798836986 CE Loss 0.4567351232034079\n",
      "EPOCH 6\n",
      "loss sup : 0.29796281456947327 loss unsup : 0.05928601324558258 loss : 0.2740951180458069 iou train : 0.5004635248531967\n",
      "EP: 6  combine loss train: 0.4066092110889931  pixel accuracy between masks  0.9186119685507473\n",
      "EP: 6 iou: 0.5004635248531967 Accuracy: 0.8755413496759651 Loss CE 0.4267852170620399\n",
      "Mean Pixel Accuracy between masks : 0.9000589318506428 Loss Validation : 0.25727317\n",
      "EPOCH 7\n",
      "loss sup : 0.23418152332305908 loss unsup : 0.2742595374584198 loss : 0.23818932473659515 iou train : 0.46822329776879396\n",
      "EP: 7  combine loss train: 0.38400970418304065  pixel accuracy between masks  0.9194580961083673\n",
      "EP: 7 iou: 0.46822329776879396 Accuracy: 0.8649345651561371 Loss CE 0.4523123662540878\n",
      "EPOCH 8\n",
      "loss sup : 0.5378691554069519 loss unsup : 0.3320249915122986 loss : 0.5172847509384155 iou train : 0.4489029876160648\n",
      "EP: 8  combine loss train: 0.38989090633526907  pixel accuracy between masks  0.9191308195653027\n",
      "EP: 8 iou: 0.4489029876160648 Accuracy: 0.8632573040578894 Loss CE 0.47334192765219774\n",
      "EPOCH 9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c490c1257bed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss_sup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_supervised\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss_sup\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss_equiv\u001b[0m \u001b[0;31m# combine loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "#torch.backends.cudnn.benchmark=True\n",
    "for ep in range(n_epochs):\n",
    "    #dataloader_train_sup = torch.utils.data.DataLoader(train_dataset_sup, batch_size=batch_size,\\\n",
    "                                                      # shuffle=True,drop_last=True)\n",
    "    dataloader_train_equiv = torch.utils.data.DataLoader(train_dataset_unsup,batch_size=batch_size,\\\n",
    "                                                     shuffle=True,drop_last=True)\n",
    "    print(\"EPOCH\",ep)\n",
    "    model.train()\n",
    "    if desactivate_bn : \n",
    "        model.apply(deactivate_batchnorm) # Disable BN \n",
    "    for batch_sup,batch_unsup in zip(dataloader_train_sup,dataloader_train_equiv):\n",
    "        optimizer.zero_grad()\n",
    "        if random.random() > 0.5: # I use this to rotate the image on the left and on the right during training.\n",
    "            angle = np.random.randint(0,angle_max)\n",
    "        else:\n",
    "            angle = np.random.randint(360-angle_max,360)\n",
    "        x_unsup,_ = batch_unsup\n",
    "        loss_equiv,acc = compute_transformations_batch(x_unsup,model,angle,reshape=False,\\\n",
    "                                                     criterion=criterion_unsupervised,Loss = Loss,\\\n",
    "                                                       device=device)\n",
    "        x,mask = batch_sup\n",
    "        x = x.to(device)\n",
    "        mask = mask.to(device)\n",
    "        pred = model(x)[\"out\"]\n",
    "        loss_equiv = loss_equiv.to(device) # otherwise bug in combining the loss \n",
    "        loss_sup = criterion_supervised(pred,mask)\n",
    "        loss = gamma*loss_sup + (1-gamma)*loss_equiv # combine loss              \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # append for plot\n",
    "        all_pix_accuracy.append(acc) # accuracy between the original mask and the transform mask put back in place\n",
    "        all_loss_train_unsup.append(loss_equiv.item())\n",
    "        all_loss_train_sup.append(loss_sup.item())\n",
    "        all_combine_loss_train.append(loss.item())\n",
    "            \n",
    "    #lr_scheduler.step()\n",
    "    #\n",
    "    # Eval IoU Train \n",
    "    state = ev.eval_model(model,dataloader_val,device=device,num_classes=21)\n",
    "    iou = state.metrics['mean IoU']\n",
    "    iou_train.append(iou)\n",
    "    m_loss_combine = np.array(all_combine_loss_train).mean()\n",
    "    m_acc = np.array(all_pix_accuracy).mean()\n",
    "    combine_loss_train.append(m_loss_combine)\n",
    "    pix_accuracy_train.append(m_acc)\n",
    "    loss_train_sup.append(np.array(all_loss_train_sup).mean())\n",
    "    loss_train_unsup.append(np.array(all_loss_train_unsup).mean())\n",
    "\n",
    "    all_pix_accuracy = []\n",
    "    all_loss_train_unsup = []\n",
    "    all_loss_train_sup = []\n",
    "    all_combine_loss_train = []\n",
    "    print(\"loss sup :\",loss_sup.item(),\"loss unsup :\",loss_equiv.item(),\"loss :\",loss.item(),\"iou train :\",iou) \n",
    "    print(\"EP:\",ep,\" combine loss train:\",m_loss_combine,\" pixel accuracy between masks \",m_acc)\n",
    "\n",
    "    ## Evaluate the  model\n",
    "    model.eval()\n",
    "    state = ev.eval_model(model,dataloader_val,device=device,num_classes=21)\n",
    "    iou = state.metrics['mean IoU']\n",
    "    acc = state.metrics['accuracy']\n",
    "    loss = state.metrics['CE Loss']\n",
    "    print('EP:',ep,'iou:',state.metrics['mean IoU'],\\\n",
    "          'Accuracy:',state.metrics['accuracy'],'Loss CE',state.metrics['CE Loss'])\n",
    "    loss_test.append(loss)\n",
    "    iou_test.append(iou)\n",
    "    accuracy_test.append(acc)\n",
    "    if ep%3==0: # Eval loss equiv and equivariance accuracy for the validation dataset\n",
    "        m_pix_acc, m_loss_equiv = eval_accuracy_equiv(model,dataloader_val,criterion=criterion_unsupervised,\\\n",
    "                        nclass=21,device=device,Loss=Loss,plot=False,angle_max=30,random_angle=False)\n",
    "        loss_test_unsup.append(m_loss_equiv)\n",
    "        pix_accuracy_test.append(m_pix_acc)    \n",
    "    if ep%5==0: # Eval IoU on rotated input images\n",
    "        print('VOC Dataset Train')\n",
    "        d_iou = ev.eval_model_all_angle(model,train=True,batch_size=batch_size,device=device,num_classes=21)\n",
    "        print('VOC Dataset Val')\n",
    "        d_iou = ev.eval_model_all_angle(model,train=False,batch_size=batch_size,device=device,num_classes=21)\n",
    "    ## Save model\n",
    "    if save_all_ep:\n",
    "        save_model = model_name+'_ep'+str(ep)+'.pt'\n",
    "        save = os.path.join(SAVE_DIR,save_model)\n",
    "        torch.save(model,save)\n",
    "    else:\n",
    "        save_model = model_name+'.pt'\n",
    "        save = os.path.join(SAVE_DIR,save_model)\n",
    "        torch.save(model,save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "print(\"EVAL ON TRAIN VOC 2012:\")\n",
    "m_pix_acc, m_loss_equiv = eval_accuracy_equiv(model,dataloader_train_VOC,criterion=criterion_unsupervised,\\\n",
    "                        nclass=21,device=device,Loss=Loss,plot=False,angle_max=30,random_angle=False) \n",
    "d_iou = ev.eval_model_all_angle(model,train=True,batch_size=batch_size,device=device,num_classes=21)\n",
    "print(\"EVAL ON VAL VOC 2012:\")\n",
    "m_pix_acc, m_loss_equiv = eval_accuracy_equiv(model,dataloader_val,criterion=criterion_unsupervised,\\\n",
    "                        nclass=21,device=device,Loss=Loss,plot=False,angle_max=30,random_angle=False) \n",
    "d_iou = ev.eval_model_all_angle(model,train=False,batch_size=batch_size,device=device,num_classes=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAUmF-R0B6U0"
   },
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ac8jahWTB7gq",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title(\"FCN KL semisup. Combine loss train\")\n",
    "plt.plot(combine_loss_train)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title(\"FCN KL semisup. Equivariance loss train\")\n",
    "plt.plot(loss_train_unsup)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title(\"FCN KL semisup. CE loss train\")\n",
    "plt.plot(loss_train_sup)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title(\"FCN KL semisup. Equivariance Accuracy train\")\n",
    "plt.plot(pix_accuracy_train)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title(\"FCN KL semisup. Mean iou train \")\n",
    "plt.plot(iou_train)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Mean IOU\")\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title(\"FCN KL semisup. Cross entropy loss test\")\n",
    "plt.plot(loss_test)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title(\"FCN KL semisup. Equivariance loss test\")\n",
    "plt.plot(loss_test_unsup)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title(\"FCN KL semisup. Equivariance accuracy test\")\n",
    "plt.plot(pix_accuracy_test)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title(\"FCN KL  semisup. Mean iou test\")\n",
    "plt.plot(iou_test)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Mean IOU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FCN_seg2012.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
