{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = '/data/voc2012'\n",
    "SAVE_DIR = '/data/model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor_target(img):\n",
    "  img = np.array(img)\n",
    "  # border\n",
    "  img[img==255] = 0 # border = background \n",
    "  return torch.LongTensor(img)\n",
    "\n",
    "\n",
    "size = (480,480)\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "transform_input = transforms.Compose([\n",
    "                                   transforms.Resize(size),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                                    ])\n",
    "\n",
    "transform_mask = transforms.Compose([\n",
    "                                   transforms.Resize(size),\n",
    "                                   transforms.Lambda(to_tensor_target)\n",
    "                                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dset.VOCSegmentation(dataroot,year='2012', image_set='train', download=True,\n",
    "                                     transform= transform_input,\n",
    "                                     target_transform= transform_mask)\n",
    "\n",
    "val_dataset = dset.VOCSegmentation(dataroot,year='2012', image_set='val', download=True,\n",
    "                                     transform= transform_input,\n",
    "                                     target_transform= transform_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "dataloader_val = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device :\",device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /data/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(file=None):\n",
    "    if file is None:\n",
    "        model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True)\n",
    "        model.to(device)\n",
    "    else:\n",
    "        model = torch.load(os.path.join(SAVE_DIR,file))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('fcn_voc_sbd_unsup_g0_L1.pt')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_per_classes =\\\n",
    "eval_accuracy_equiv(model,dataloader_val,criterion=nn.KLDivLoss(reduction='mean'),nclass=21,device=device,plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_per_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
