{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZbrYjvKvkaH"
   },
   "source": [
    "# FCN VOC 2012 and SBD Semi Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KX2t25zgvkaf"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torch.utils.data as tud\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from sklearn.metrics import jaccard_score\n",
    "import pickle\n",
    "import my_datasets as mdset\n",
    "import eval_train as ev\n",
    "from utils import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aw2xTNEkvka0"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Dataset : Pascal VOC 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IzkSq_mgrcLR"
   },
   "outputs": [],
   "source": [
    "dataroot_voc = '/data/voc2012'\n",
    "dataroot_sbd = '/data/sbd'\n",
    "model_name = 'fcn_voc_sbd30_semisup_g05_CE'\n",
    "SAVE_DIR = '/data/model'\n",
    "save = os.path.join(SAVE_DIR,model_name)\n",
    "batch_size = 2\n",
    "gamma = 0.5\n",
    "save_all_ep = True\n",
    "#criterion_unsupervised = nn.L1Loss(reduction='mean')\n",
    "#criterion_unsupervised = nn.KLDivLoss(reduction = 'batchmean', log_target = False)\n",
    "criterion_unsupervised = nn.CrossEntropyLoss(ignore_index=21)\n",
    "criterion_supervised = nn.CrossEntropyLoss(ignore_index=21) # On ignore la classe border.\n",
    "Loss = 'CE' # Loss = 'KL' or 'CE' or None for L1,MSEâ€¦\n",
    "rotate = False\n",
    "split = True\n",
    "fully_supervised = True # Use the same dataloader for equivariance loss and supervised loss\n",
    "n_epochs_supervised = 25 # Train in fully supervised for n_epoch_supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "AAj55UZxOZte",
    "outputId": "9257ab6a-d935-4c69-ddf6-ea40effa907a"
   },
   "outputs": [],
   "source": [
    "train_dataset_VOC = mdset.VOCSegmentation(dataroot_voc,year='2012', image_set='train', download=True,rotate=rotate)\n",
    "val_dataset_VOC = mdset.VOCSegmentation(dataroot_voc,year='2012', image_set='val', download=True)\n",
    "train_dataset_SBD = mdset.SBDataset(dataroot_sbd, image_set='train_noval',mode='segmentation',rotate=rotate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatene Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_unsup = tud.ConcatDataset([train_dataset_VOC,train_dataset_SBD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split:\n",
    "    train_dataset_sup = split_dataset(train_dataset_unsup,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LUqeDj5hvka3",
    "outputId": "92a782a5-3bc5-48f5-b57a-af5993c9c201"
   },
   "outputs": [],
   "source": [
    "if fully_supervised : \n",
    "    train_dataset_unsup = train_dataset_sup\n",
    "    \n",
    "dataloader_train_sup = torch.utils.data.DataLoader(train_dataset_sup, batch_size=batch_size,\\\n",
    "                                                       shuffle=True,drop_last=True)\n",
    "dataloader_train_equiv = torch.utils.data.DataLoader(train_dataset_unsup,batch_size=batch_size,\\\n",
    "                                                     shuffle=True,drop_last=True)\n",
    "\n",
    "dataloader_val = torch.utils.data.DataLoader(val_dataset_VOC, batch_size=batch_size)\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device :\",device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "63IQP70Qg8BL",
    "outputId": "e38a98ff-0b99-4e3a-eae4-bdf29ddf3636",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Taille dataset train supervised :\",len(train_dataset_sup))\n",
    "print(\"Taille dataset train unsupervised :\",len(train_dataset_unsup))\n",
    "print(\"Taille dataset val VOC :\",len(val_dataset_VOC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tl0nR2NJ8I2i"
   },
   "source": [
    "\n",
    "## FCN Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(file=None,fcn=False,pretrained=False):\n",
    "    if file is None:\n",
    "        if fcn is False:\n",
    "            model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=pretrained)\n",
    "        else:\n",
    "            model = torchvision.models.segmentation.fcn_resnet101(pretrained=pretrained)\n",
    "    else:\n",
    "        model = torch.load(os.path.join(SAVE_DIR,file))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(fcn=True,pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "colab_type": "code",
    "id": "N3HOA_uv8Oyd",
    "outputId": "c886c10b-558e-4acb-a2ac-b2110fddc29f"
   },
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NQeaeCk1hsGu"
   },
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gABxDT_6hrTf"
   },
   "outputs": [],
   "source": [
    "learning_rate = 10e-4\n",
    "moment = 0.9\n",
    "wd = 2e-4\n",
    "n_epochs = 26\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=moment, weight_decay=wd)\n",
    "angle_max = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_train = []\n",
    "iou_test = []\n",
    "combine_loss_train = []\n",
    "combine_loss_test = []\n",
    "loss_train_unsup = []\n",
    "loss_train_sup = []\n",
    "loss_test = []\n",
    "loss_test_unsup = []\n",
    "pix_accuracy_train = []\n",
    "pix_accuracy_test = []\n",
    "accuracy_test = []\n",
    "accuracy_train = []\n",
    "#\n",
    "all_combine_loss_train = []\n",
    "all_loss_train_sup = []\n",
    "all_loss_train_unsup = []\n",
    "all_iou_train= []\n",
    "all_pix_accuracy =  []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain fully supervised "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pretrain the model in fully supervised\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for ep in range(n_epochs_supervised):\n",
    "    print(\"EPOCH\",ep)\n",
    "    model.train()\n",
    "    for i,(x,mask) in enumerate(dataloader_train_sup):\n",
    "        x = x.to(device)\n",
    "        mask = mask.to(device)  \n",
    "        pred = model(x)\n",
    "        pred = pred[\"out\"]\n",
    "        loss = criterion_supervised(pred,mask)\n",
    "        all_loss_train_sup.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        all_iou_train.append(inter_over_union(pred.argmax(dim=1).detach().cpu(),mask.detach().cpu()))\n",
    "        optimizer.step()        \n",
    "        \n",
    "    #lr_scheduler.step()\n",
    "    m_iou = np.array(all_iou_train).mean()\n",
    "    m_loss = np.array(all_loss_train_sup).mean()\n",
    "    loss_train_sup.append(m_loss)\n",
    "    iou_train.append(m_iou)\n",
    "    all_loss_train_sup = []\n",
    "    all_iou_train = []\n",
    "    print(\"EP:\",ep,\" loss train:\",m_loss,\" iou train:\",m_iou)\n",
    "    \n",
    "    #Eval model\n",
    "    \n",
    "    model.eval()\n",
    "    state = ev.eval_model(model,dataloader_val,device=device,num_classes=21)\n",
    "    iou = state.metrics['mean IoU']\n",
    "    acc = state.metrics['accuracy']\n",
    "    loss = state.metrics['CE Loss'] \n",
    "    loss_test.append(loss)\n",
    "    iou_test.append(iou)\n",
    "    accuracy_test.append(acc)\n",
    "    print('EP:',ep,'iou:',state.metrics['mean IoU'],\\\n",
    "          'Accuracy:',state.metrics['accuracy'],'Loss CE',state.metrics['CE Loss'])\n",
    "    \n",
    "    torch.save(model,save)\n",
    "    ## Save model\n",
    "    if save_all_ep:\n",
    "        save_model = model_name+'_pretrain_sup'+'_ep'+str(ep)+'.pt'\n",
    "        save = os.path.join(SAVE_DIR,save_model)\n",
    "        torch.save(model,save)\n",
    "    else:\n",
    "        save_model = model_name+'_pretrain_sup'+'.pt'\n",
    "        save = os.path.join(SAVE_DIR,save_model)\n",
    "        torch.save(model,save)\n",
    "    \n",
    "m_pix_acc, m_loss_equiv = eval_accuracy_equiv(model,dataloader_val,\\\n",
    "                    criterion=criterion_unsupervised,nclass=21,device=device,plot=False) \n",
    "d_iou = ev.eval_model_all_angle(model,batch_size=batch_size,device=device,num_classes=21)\n",
    "for k in d_iou.keys():\n",
    "    print('Scores for datasets rotate by',k,'degrees:')\n",
    "    print('   mIoU',d_iou[k]['mIoU'],'Accuracy',d_iou[k]['Accuracy'],'CE Loss',d_iou[k]['CE Loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi sup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2*10e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5Y_Uq45Mh0kb",
    "outputId": "028dcf3b-6255-4f1a-dba0-b8ddd0179513"
   },
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "for ep in range(n_epochs):\n",
    "    #dataloader_train_sup = torch.utils.data.DataLoader(train_dataset_sup, batch_size=batch_size,\\\n",
    "                                                      # shuffle=True,drop_last=True)\n",
    "    #dataloader_train_equiv = torch.utils.data.DataLoader(train_dataset_unsup,batch_size=batch_size,\\\n",
    "                                                     #shuffle=True,drop_last=True)\n",
    "    print(\"EPOCH\",ep)\n",
    "    model.train()\n",
    "\n",
    "    for batch_sup,batch_unsup in zip(dataloader_train_sup,dataloader_train_equiv):\n",
    "        optimizer.zero_grad()\n",
    "        if random.random() > 0.5: # I use this to rotate the image on the left and on the right during training.\n",
    "            angle = np.random.randint(0,angle_max)\n",
    "        else:\n",
    "            angle = np.random.randint(360-angle_max,360)\n",
    "        x_unsup,_ = batch_unsup\n",
    "        loss_equiv,acc = compute_transformations_batch(x_unsup,model,angle,reshape=False,\\\n",
    "                                                     criterion=criterion_unsupervised,Loss = Loss,\\\n",
    "                                                       device=device)\n",
    "        x,mask = batch_sup\n",
    "        x = x.to(device)\n",
    "        mask = mask.to(device)\n",
    "        pred = model(x)[\"out\"]\n",
    "        loss_equiv = loss_equiv.to(device) # otherwise bug in combining the loss \n",
    "        loss_sup = criterion_supervised(pred,mask)\n",
    "        loss = gamma*loss_sup + (1-gamma)*loss_equiv # combine loss              \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # append for plot\n",
    "        all_pix_accuracy.append(acc) # accuracy between the original mask and the transform mask put back in place\n",
    "        all_loss_train_unsup.append(loss_equiv.item())\n",
    "        all_loss_train_sup.append(loss_sup.item())\n",
    "        all_combine_loss_train.append(loss.item())\n",
    "            \n",
    "    #lr_scheduler.step()\n",
    "    #\n",
    "    m_loss_combine = np.array(all_combine_loss_train).mean()\n",
    "    m_acc = np.array(all_pix_accuracy).mean()\n",
    "    combine_loss_train.append(m_loss_combine)\n",
    "    pix_accuracy_train.append(m_acc)\n",
    "    loss_train_sup.append(np.array(all_loss_train_sup).mean())\n",
    "    loss_train_unsup.append(np.array(all_loss_train_unsup).mean())\n",
    "\n",
    "    all_pix_accuracy = []\n",
    "    all_loss_train_unsup = []\n",
    "    all_loss_train_sup = []\n",
    "    all_combine_loss_train = []\n",
    "    print(\"loss sup :\",loss_sup.item(),\"loss unsup\",loss_equiv.item(),\"loss\",loss.item()) \n",
    "    print(\"EP:\",ep,\" combine loss train:\",m_loss_combine,\" pixel accuracy between masks \",m_acc)\n",
    "\n",
    "    ## Evaluate the  model\n",
    "    model.eval()\n",
    "    state = ev.eval_model(model,dataloader_val,device=device,num_classes=21)\n",
    "    iou = state.metrics['mean IoU']\n",
    "    acc = state.metrics['accuracy']\n",
    "    loss = state.metrics['CE Loss']\n",
    "    print('EP:',ep,'iou:',state.metrics['mean IoU'],\\\n",
    "          'Accuracy:',state.metrics['accuracy'],'Loss CE',state.metrics['CE Loss'])\n",
    "    loss_test.append(loss)\n",
    "    iou_test.append(iou)\n",
    "    accuracy_test.append(acc)\n",
    "    if ep%3==0:\n",
    "        m_pix_acc, m_loss_equiv = eval_accuracy_equiv(model,dataloader_val,\\\n",
    "                    criterion=criterion_unsupervised,nclass=21,device=device,plot=False)\n",
    "        loss_test_unsup.append(m_loss_equiv)\n",
    "        pix_accuracy_test.append(m_pix_acc)\n",
    "        \n",
    "    if ep%5==0:\n",
    "        d_iou = ev.eval_model_all_angle(model,batch_size=batch_size,device=device,num_classes=21)\n",
    "        for k in d_iou.keys():\n",
    "            print('Scores for datasets rotate by',k,'degrees:')\n",
    "            print('   mIoU',d_iou[k]['mIoU'],'Accuracy',d_iou[k]['Accuracy'],'CE Loss',d_iou[k]['CE Loss'])\n",
    "        \n",
    "        \n",
    "    ## Save model\n",
    "    if save_all_ep:\n",
    "        save_model = model_name+'_ep'+str(ep)+'.pt'\n",
    "        save = os.path.join(SAVE_DIR,save_model)\n",
    "        torch.save(model,save)\n",
    "    else:\n",
    "        save_model = model_name+'.pt'\n",
    "        save = os.path.join(SAVE_DIR,save_model)\n",
    "        torch.save(model,save)\n",
    "    \n",
    "m_pix_acc, m_loss_equiv = eval_accuracy_equiv(model,dataloader_val,\\\n",
    "                    criterion=criterion_unsupervised,nclass=21,device=device,plot=False) \n",
    "d_iou = ev.eval_model_all_angle(model,batch_size=batch_size,device=device,num_classes=21)\n",
    "for k in d_iou.keys():\n",
    "    print('Scores for datasets rotate by',k,'degrees:')\n",
    "    print('   mIoU',d_iou[k]['mIoU'],'Accuracy',d_iou[k]['Accuracy'],'CE Loss',d_iou[k]['CE Loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAUmF-R0B6U0"
   },
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ac8jahWTB7gq"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title(\"FCN L1 semi sup. Combine loss train\")\n",
    "plt.plot(combine_loss_train)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title(\"FCN L1 semi sup. Equivariance loss train\")\n",
    "plt.plot(loss_train_unsup)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title(\"FCN L1 semi sup. CE loss train\")\n",
    "plt.plot(loss_train_sup)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title(\"FCN L1 semi sup. Equivariance Accuracy train\")\n",
    "plt.plot(pix_accuracy_train)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title(\"FCN L1 semi sup. Mean iou train \")\n",
    "plt.plot(iou_train)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Mean IOU\")\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title(\"FCN L1 semi sup. Cross entropy loss test\")\n",
    "plt.plot(loss_test)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title(\"FCN L1 semi sup. Equivariance loss test\")\n",
    "plt.plot(loss_test_unsup)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title(\"FCN L1 semi sup. Equivariance accuracy test\")\n",
    "plt.plot(pix_accuracy_test)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title(\"FCN L1 semi sup. Mean iou test\")\n",
    "plt.plot(iou_test)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Mean IOU\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FCN_seg2012.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
